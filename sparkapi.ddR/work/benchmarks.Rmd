Wed Aug 10 09:19:05 KST 2016

Indrajit:

> We should avoid the situation where you have features beyond SparkR
> dataframes but people donâ€™t use them due to poor performance. A quick
> solution is to add a suite of performance unit tests that cover small and
> medium sized datasets.

I'm pretty worried about performance and usability of this thing. 
Performance in terms of how long it will take to run, usability in terms of
how feasible it is to use this on anything other than simulated data.
Before I proceed further down this path I'm going to do a few benchmarks
to see if I can get some rough estimates for the bounds on performance.

`do_mapply` is the heart of ddR; everything uses it. Right now my suspicion
based on casual use
is that the version of `mapply` I've implemented on Spark is going to be to
expensive, implying that `do_mapply` will be expensive. Then to get some
reasonable lower bound for performance we can check:

1) How many calls to `do_mapply` happen in various operations.
2) How expensive is a single call to `mapply` on Spark (best and worse
   case).


### How many calls to `do_mapply` happen in various operations?

Following [this
example](http://stackoverflow.com/questions/21687514/r-count-function-calls)
use a global count. This should be fine as long as it runs sequentially.

```

library(ddR)

count_do_dmapply = 0L
count_do_collect = 0L

plusone = function(counter_name){
    count = get(counter_name, .GlobalEnv)
    count = count + 1L
    assign(counter_name, count, .GlobalEnv)
}

trace(do_dmapply, tracer = function() plusone("count_do_dmapply")
      , print = FALSE)
trace(do_collect, tracer = function() plusone("count_do_collect")
      , print = FALSE)

counter = function(code_text){
    assign("count_do_dmapply", 0L, .GlobalEnv)
    assign("count_do_collect", 0L, .GlobalEnv)
    eval(parse(text = code_text), envir = .GlobalEnv)
    data.frame(code = code_text
               , do_dmapply = get("count_do_dmapply", .GlobalEnv)
               , do_collect = get("count_do_collect", .GlobalEnv)
               , stringsAsFactors = FALSE
               )
}

count_all = function(codevec){
    do.call(rbind, lapply(codevec, counter))
}


```

### Initializing objects

```

# A million random numbers
a <- matrix(rnorm(1000000), nrow=1000)

init_code = c("dl <- dlist(1:10, letters, runif(10))"
    , "df <- as.dframe(iris)"
    , "df2 <- as.dframe(iris, psize=c(5, 2))"
    , "dfnum <- as.dframe(iris[, 1:4], psize=c(5, 2))"
    , "da <- as.darray(a, psize = c(100, 100))"
    )

counter(init_code[1])

init_counts = count_all(init_code)

```

### Methods

```

# Dataframes
methods(class = class(df))

ops = count_all(c("df[1:5, 1:2]"
    , "df$Species"
    , "colMeans(dfnum)"
    , "rowSums(dfnum)"
    , "mean(dfnum)"
    , "da[1:300, 357:498]"
    , "colMeans(da)"
    , "rowSums(da)"
    , "mean(da)"
    #, "rbind(da, da)" # Fails, but for a different reason?
))

ops

```

Ah, dlists, darrays, and dframes are all the same class. But how does OO programming work then?

### Functions

```

ddR_funs = count_all(c("collect(df)"
    , "collect(da)"
    , "collect(dl)"
    , "repartition(df, df2)"
    , "dlapply(dl, head)"
))

ddR_funs

```

So it seems that most things in the base library take 1 or 2 calls. This
should not be a problem. How about the algorithms?

### Algorithms

Copying from docs:

```

# The trace is still turned on from before
count_do_dmapply = 0L
count_do_collect = 0L

library(glm.ddR)
nInst = 2 # Change level of parallelism
useBackend(parallel,executors = nInst)
ncol = 200
nrow = 100000
nrow = as.integer(nrow/nInst)
coefficients = 10*matrix(rnorm(ncol),nrow = ncol)
generateGLMFeatures <- function(id, nrow, ncol) {
    matrix(rnorm(nrow*ncol),nrow = nrow,ncol = ncol)
}
generateGLMResponses <- function(features, coefficients) {
    matrix(features %*% coefficients, ncol = 1)
}
x <- dmapply(generateGLMFeatures,id = 1:nInst,
                MoreArgs = list(nrow = nrow, ncol = ncol),
        output.type = "darray", 
        combine = "rbind", nparts = c(nInst,1))
y <- dmapply(generateGLMResponses,features = parts(x),
                MoreArgs = list(coefficients = coefficients),
        output.type = "darray", 
        combine = "rbind", nparts = c(nInst,1))

training_time <- system.time({model <- dglm(responses = y, predictors = x,
completeModel = TRUE)})[3]
cat("training dglm model on distributed data: ", training_time," sec \n")
prediction_time <- system.time({predictions <- predict(model, x)})[3]
cat("predicting from dglm model on same distributed data: ", prediction_time," sec \n")

glm_do_dmapply = count_do_dmapply
glm_do_collect = count_do_collect

```

35 and 13 for this run of GLM.

```

# The trace is still turned on from before
count_do_dmapply = 0L
count_do_collect = 0L

library(kmeans.ddR)
nInst = 2
useBackend(parallel,executors = nInst)
ncol = 100
nrow = 1000000
nrow = as.integer(nrow/nInst)
K = 10
centers = 100*matrix(rnorm(K*ncol),nrow = K)
generateRFData <- function(id, centers, nrow, ncol) {
    offsets = matrix(rnorm(nrow*ncol),nrow = nrow,ncol = ncol)
    cluster_ids = sample.int(nrow(centers),nrow,replace = TRUE)
    feature_obs = centers[cluster_ids,] + offsets
    features <- data.frame(cluster_ids, feature_obs)
}
features <- dmapply(generateRFData,id = 1:nInst,
                MoreArgs = list(centers = centers, nrow = nrow, ncol = ncol),
        output.type = "dframe", 
        combine = "rbind", nparts = c(nInst,1))
colnames(features) <- paste("X",1:ncol(features),sep="")
training_time <- system.time({model <- dkmeans(features,K)})[3]
cat("training dkmeans model on distributed data: ", training_time,"\n")

glm_do_dmapply = count_do_dmapply
glm_do_collect = count_do_collect

```
