# Design Document

# Spark 

We need to execute user defined functions on local dataframes, matrices, and lists.
Dataframes and matrices can be partitioned by both rows and columns. The
underlying idea of Spark dataframes is RDD's consisting of rows, which is
not compatible with partitioning the columns. 

In order to use Spark in this more general way the idea is to have it
simply act as a key value pair which stores serialized binary R objects and
can run R code:

index   |   BLOB 
--------|-------
1       |   XXX
2       |   XXX
...     |   ...

There will be a corresponding internal local R table to do the lookups:

row     |   column  | index
--------|-----------|-----
1       |   1       | 1
2       |   1       | 2
...     |   ...

__Pros__

- This is a general design that would work for many systems, and
  possibly even the serialized version

- Resiliency, persistence, caching, etc. all provided by Spark

__Cons__

- Not using Spark's ability to store data in anything other than a binary
  format. However, once the basic functionality is present it might be
  possible to extend this to store numeric arrays.



## Dependencies

There are two options for packages that could be depended on to
implement Spark as a backend.

### sparkapi

With this lower level approach we can store the distributed object directly
in a [Pair
RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaPairRDD)

Having direct access us to the underlying RDD allows operations like
`do_collect(x, parts)`. Existing SparkR code with RDD's did not allow
specifying the `parts`.

__Pros__

- Pair RDD seems like a natural container for this
- Interacting directly with Spark R API layer
- Low level access into any general methods on the RDD we may need for repartitioning,
  etc.

__Cons__

- Requires bringing in and maintaining a minimal set of private SparkR
  code, like Javier's example
- Requires more knowledge of Spark / Scala / Java
- Probably will take longer to develop

Wed Jul 27 11:13:34 PDT 2016

The first idea is to make a pair RDD that has `lapply` and `[[` methods.
This differs from SparkR RDD's since it has `[[` and it doesn't build
pipelines in R, so it's simpler.

Conceptually, it's only necessary to do the actual computation once a user
calls `[[` or a `collect` method which would return the evaluated RDD as a
local R list. This lazy evaluation is what currently happens.

Caching is off by default. But I'm going to turn it on for everything
because the current implementation for ddR does lots of collecting and
calling length.

### SparkR

This approach consists of storing the data in a table as above, and then
using SparkR's
[dapply](http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs-updated/sparkr.html#applying-user-defined-function)
to launch the R processes on the workers.

Mon Jul 25 08:15:36 PDT 2016

The idea here was to work with R objects stored in either a binary column or
a column of arrays. In a local R data.frame these are list columns. I
wasn't able to get `SparkR::dapply` to return list columns, so until this
is possible this approach won't work.

__Pros__

- Building on DataFrames provides compatibility that may make many things
  easier, such as loading and saving data.
- Relying more on Apache Spark maintained code

__Cons__

- More difficult for users to install since SparkR is not on CRAN
- Long term stability and support of necessary code in SparkR is not
  guaranteed
- May be more overhead from running through SparkR 


# Methods

A ddR backend should implement the following methods:

## object.R

__initialize__ Function `new` calls this to create a new parallel object

__combine__ 

```
a = dlist(1:5, 2)
b = dlist(1:10)

combine(ddR:::ddR.env$driver, list(a, b))

Error in if (!is.dlist(dobj)) ans <- t(ans) :
  missing value where TRUE/FALSE needed
```

__get_parts__ 

Only used in one place:

```
~/dev/ddR/ddR/R $ grep "get_parts(" *
dobject.R:  partitions <- get_parts(dobj, index)
```

TODO: understand `combine` and `getparts`. I'm not having any luck using
these with default parallel backend.

__do_collect__ Move from storage to local list, matrix, or data.frame in
calling context. Also can extract the ith element.

## driver.R

This file creates the variable `sparkapi.ddR.env`, an environment which is
used to hold anything you like specific to the backend.

TODO: Clarify roles and relationships of (ddR.env, sparkapi.ddR.env, driver objects)

__init__ Called when the backend driver is initialized.

Looking at the code it appears that it should return the number of
executors so that the `ddR.env$nexecutors` is correctly set.

TODO: Document this return value in ddR.

__shutdown__ Called when the backend driver is shutdown.

__do_dmapply__  Backend-specific dmapply logic. This is a required override for all
backends to implement so dmapply works.

#### Usage

```
do_dmapply(driver, func, ..., MoreArgs = list(), output.type = "dlist",
nparts = NULL, combine = "default")
```

#### Arguments

- `driver` The driver that the logic dispatches on.
- `func` The function to execute
- `...` Iterable arguments from dmapply.
- `MoreArgs` A list of more arguments to the function.
- `output.type` The type of output (can be ’dlist’, ’darray’, ’sparse_darray’,
        or ’dframe’).
- `nparts` A 2d-vector indicating how the output is partitioned.
- `combine` One of ’default’, ’rbind’, ’cbind’, or ’c’, which specifies how the
        results from each partition should be combined.

#### Value 

An object specific to the backend, with the nparts and psize fields filled.

#### Notes

There is only one call to `do dmapply` inside `dmapply`. `dmapply`
basically just does bookkeeping like dimension checking.

Use the parallel backend as the reference implementation:

```

library(ddR)

a = 1:5
yd = dmapply(sum, a, a + 1, a + 2)
y = collect(yd)
y2 = mapply(sum, a, a + 1, a + 2)
# Work similarly

dmapply(function(x, y) x, a, a + 1)

```

To make this work in Spark the approach is to convert all the arguments to
RDD's, zip them together, and then do a map on the zipped RDD.

## Slow going with mapply

Back to the drawing board with `mapply`. The trouble is that in Spark zipping
RRDD's and using lapply is a commutative operation. Ie. these return the
same thing:
```
lapply(zip(a, b), FUN)
zip(lapply(a, FUN), lapply(b, FUN))
```

In local R this is not the case, because zip creates nested lists

```
zip(a, b) -> [[a1, b1], [a2, b2], ..., [an, bn]]
```

So the function `FUN` must operate on lists containing the elements, rather
than the elements themselves.

Last week I attempted to get around this by first zipping, collecting and then using
`org.apache.spark.api.r.RRDD.createRDDFromArray`. This works, but it's
wrong because it goes from an RDD -> a local Java array on the master,
meaning that all the parallelism, resiliency, etc from Spark is gone. It's
like collecting back into R and loading back into Spark. 

```
RDD -> Java Array on master -> RDD    :( Why even use Spark then?
```

However, SparkR is able to do this. The relevant (abridged) code for binary
objects is their
`mergePartitions` function:

```
mergePartitions <- function(zipped_rdd){
  partitionFunc <- function(partIndex, part) {
      keys <- part[1 : (lengthOfKeys - 1)]
      values <- part[ (lengthOfKeys + 1) : (len - 1) ]
      mapply(
        function(k, v) { list(k, v) },
        keys,
        values,
        SIMPLIFY = FALSE,
        USE.NAMES = FALSE)
  }
  PipelinedRDD(zipped_rdd, partitionFunc)
}
```

Judging by the other code, `part` is a list. 

Ok, seems like I got over the problem. The crux is to understand exactly
what kind of function `org.apache.spark.api.r.RRDD` expects and how it
operates. I haven't seen this in any docs- just learning it through
experiment and reading source code.

# Spark R API

The class
[`org.apache.spark.api.r.RRDD`](http://spark.apache.org/docs/latest/api/java/org/apache/spark/api/r/RRDD.html)
is the main way to interact directly with Spark from R. These interactions
happen through the constructor:

```
public RRDD(RDD<T> parent,
    byte[] func,
    String deserializer,
    String serializer,
    byte[] packageNames,
    Object[] broadcastVars,
    scala.reflect.ClassTag<T> evidence$4)
```

__RDD__: The parent RDD to apply the function to. When this is collected
in Spark it's a sequence of byte arrays, with each byte array corresponding
to a part which is a list in R.

__serializer / deserializer__: This can be string or bytes. For simplicity
we only use bytes.

__func__: A serialized closure (function + environment) with signature
`function(partIndex, part)`.

__packageNames__: 

__broadcastVars__: 

__evidence$4__: Don't know what this is, but have been using something
related to bytes.

### Programming model


### Misc.

Glancing through the scala source, it seems that new R processes start
every time `.compute()` is called on an RRDD. Haven't yet checked, but this
may be what the Pipelined RDD's are for.
