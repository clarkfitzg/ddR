# Design Document

# Spark 

We need to execute user defined functions on local dataframes, matrices, and lists.
Dataframes and matrices can be partitioned by both rows and columns. The
underlying idea of Spark dataframes is RDD's consisting of rows, which is
not compatible with partitioning the columns. 

In order to use Spark in this more general way the idea is to have it
simply act as a key value pair which stores serialized binary R objects and
can run R code:

index   |   BLOB 
--------|-------
1       |   XXX
2       |   XXX
...     |   ...

There will be a corresponding internal local R table to do the lookups:

row     |   column  | index
--------|-----------|-----
1       |   1       | 1
2       |   1       | 2
...     |   ...

__Pros__

- This is a general design that would work for many systems, and
  possibly even the serialized version

- Resiliency, persistence, caching, etc. all provided by Spark

__Cons__

- Not using Spark's ability to store data in anything other than a binary
  format. However, once the basic functionality is present it might be
  possible to extend this to store numeric arrays.



## Dependencies

There are two options for packages that could be depended on to
implement Spark as a backend.

### sparkapi

With this lower level approach we can store the distributed object directly
in a [Pair
RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaPairRDD)

Having direct access us to the underlying RDD allows operations like
`do_collect(x, parts)`. Existing SparkR code with RDD's did not allow
specifying the `parts`.

__Pros__

- Pair RDD seems like a natural container for this
- Interacting directly with Spark R API layer
- Low level access into any general methods on the RDD we may need for repartitioning,
  etc.

__Cons__

- Requires bringing in and maintaining a minimal set of private SparkR
  code, like Javier's example
- Requires more knowledge of Spark / Scala / Java
- Probably will take longer to develop

Wed Jul 27 11:13:34 PDT 2016

The first idea is to make a pair RDD that has `lapply` and `[[` methods.
This differs from SparkR RDD's since it has `[[` and it doesn't build
pipelines in R, so it's simpler.

Conceptually, it's only necessary to do the actual computation once a user
calls `[[` or a `collect` method which would return the evaluated RDD as a
local R list. This lazy evaluation is what currently happens.

Caching is off by default. But I'm going to turn it on for everything
because the current implementation for ddR does lots of collecting and
calling length.

### SparkR

This approach consists of storing the data in a table as above, and then
using SparkR's
[dapply](http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs-updated/sparkr.html#applying-user-defined-function)
to launch the R processes on the workers.

Mon Jul 25 08:15:36 PDT 2016

The idea here was to work with R objects stored in either a binary column or
a column of arrays. In a local R data.frame these are list columns. I
wasn't able to get `SparkR::dapply` to return list columns, so until this
is possible this approach won't work.

__Pros__

- Building on DataFrames provides compatibility that may make many things
  easier, such as loading and saving data.
- Relying more on Apache Spark maintained code

__Cons__

- More difficult for users to install since SparkR is not on CRAN
- Long term stability and support of necessary code in SparkR is not
  guaranteed
- May be more overhead from running through SparkR 


# Methods

A ddR backend should implement the following methods:

## object.R

__initialize__ Function `new` calls this to create a new parallel object

__combine__ 

```
a = dlist(1:5, 2)
b = dlist(1:10)

combine(ddR:::ddR.env$driver, list(a, b))

Error in if (!is.dlist(dobj)) ans <- t(ans) :
  missing value where TRUE/FALSE needed
```

__get_parts__ 

Only used in one place:

```
~/dev/ddR/ddR/R $ grep "get_parts(" *
dobject.R:  partitions <- get_parts(dobj, index)
```

TODO: understand `combine` and `getparts`. I'm not having any luck using
these with default parallel backend.

__do_collect__ Move from storage to local list, matrix, or data.frame in
calling context. Also can extract the ith element.

## driver.R

This file creates the variable `sparkapi.ddR.env`, an environment which is
used to hold anything you like specific to the backend.

TODO: Clarify roles and relationships of (ddR.env, sparkapi.ddR.env, driver objects)

__init__ Called when the backend driver is initialized.

Looking at the code it appears that it should return the number of
executors so that the `ddR.env$nexecutors` is correctly set.

TODO: Document this return value in ddR.

__shutdown__ Called when the backend driver is shutdown.

__do_dmapply__ 
Backend-specific dmapply logic. This is a required override for all
backends to implement so
dmapply works.

#### Usage

```
do_dmapply(driver, func, ..., MoreArgs = list(), output.type = "dlist",
nparts = NULL, combine = "default")
```

#### Arguments

- `driver` The driver that the logic dispatches on.
- `func` The function to execute
- `...` Iterable arguments from dmapply.
- `MoreArgs` A list of more arguments to the function.
- `output.type` The type of output (can be ’dlist’, ’darray’, ’sparse_darray’,
        or ’dframe’).
- `nparts` A 2d-vector indicating how the output is partitioned.
- `combine` One of ’default’, ’rbind’, ’cbind’, or ’c’, which specifies how the
        results from each partition should be combined.

#### Value 

An object specific to the backend, with the nparts and psize fields filled.

## Working Notes

There is only one call to `do dmapply` inside `dmapply`. `dmapply`
basically just does bookkeeping like dimension checking.

```

library(ddR)

x = 1:5
yd = dmapply(sum, x, x + 1, x + 2)
y = collect(yd)

y2 = mapply(sum, x, x + 1, x + 2)



```
